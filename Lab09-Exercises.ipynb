{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6b704f5-a47c-4124-8be4-a27e13ca3503",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "## Exercise 1\n",
    "\n",
    "For the dataset below:\n",
    "\n",
    "1. Plot the decision surface of the Logistic Regression algorithm.\n",
    "2. Calculate the CVLOO error for Logistic Regression.\n",
    "3. Plot the decision surface of the ID3 algorithm (with entropy and no pruning).\n",
    "4. Calculate the CVLOO error for ID3."
   ]
  },
  {
   "cell_type": "code",
   "id": "d0a7eabd-0aa1-4809-b58d-00de769ca895",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate the dataset\n",
    "X, y = make_moons(n_samples=200, noise=0.2, random_state=42)\n",
    "\n",
    "def plot_decision_boundary(X, y, model, title):\n",
    "    # Create a mesh grid\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                        np.arange(y_min, y_max, 0.02))\n",
    "    \n",
    "    # Make predictions on the mesh grid\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot the decision boundary\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.8)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.show()\n",
    "\n",
    "def calculate_cvloo_error(X, y, model):\n",
    "    loo = LeaveOneOut()\n",
    "    predictions = []\n",
    "    true_values = []\n",
    "    \n",
    "    for train_idx, test_idx in loo.split(X):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        pred = model.predict(X_test)\n",
    "        \n",
    "        predictions.extend(pred)\n",
    "        true_values.extend(y_test)\n",
    "    \n",
    "    error = 1 - accuracy_score(true_values, predictions)\n",
    "    return error\n",
    "\n",
    "# 1. Logistic Regression\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X, y)\n",
    "plot_decision_boundary(X, y, log_reg, \"Logistic Regression Decision Boundary\")\n",
    "\n",
    "# 2. Calculate CVLOO error for Logistic Regression\n",
    "log_reg_error = calculate_cvloo_error(X, y, LogisticRegression(random_state=42))\n",
    "print(f\"Logistic Regression CVLOO Error: {log_reg_error:.4f}\")\n",
    "\n",
    "# 3. ID3 (Decision Tree with entropy criterion)\n",
    "dt = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
    "dt.fit(X, y)\n",
    "plot_decision_boundary(X, y, dt, \"ID3 Decision Boundary\")\n",
    "\n",
    "# 4. Calculate CVLOO error for ID3\n",
    "id3_error = calculate_cvloo_error(X, y, DecisionTreeClassifier(criterion='entropy', random_state=42))\n",
    "print(f\"ID3 CVLOO Error: {id3_error:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "415d3052-6ae3-47c3-aa6e-8b7fec9a901c",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Given the dataset below, implement the gradient ascent formula from the lab. Starting from an initial $w=(0, 0, 0)$, apply 10 gradient ascent steps with $\\eta = 0.01$. What are the values of $w$ after the 10 steps? \n",
    "\n",
    "_Note: The component $x_0 = 1$ was already added to the dataset, so $w$ and $X$ have the same number of dimensions._"
   ]
  },
  {
   "cell_type": "code",
   "id": "20b07227-d6a6-4c6c-8253-1ea8ded7b496",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T20:16:12.408239Z",
     "start_time": "2024-11-29T20:16:12.354821Z"
    }
   },
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "X, y = make_blobs(n_samples=200, cluster_std=3, centers=2, random_state=42)\n",
    "\n",
    "def add_intercept(X):\n",
    "    \"\"\"Add 1 as the first column of X\"\"\"\n",
    "    return np.hstack((np.ones((len(X), 1)), X))\n",
    "def sigmoid(z):\n",
    "    return math.exp(z) / (1 + math.exp(z))\n",
    "\n",
    "X = add_intercept(X)\n",
    "\n",
    "learning_rate = 0.01\n",
    "w = np.array([0, 0, 0])\n",
    "\n",
    "for i in range(10):\n",
    "    gradients = np.zeros(3)\n",
    "    for j in range(3):\n",
    "        for k in range(len(X)):\n",
    "            gradients[j] += (y[k] - sigmoid(w.dot(X[k]))) * X[k][j]\n",
    "            \n",
    "    w = w + learning_rate * gradients\n",
    "        \n",
    "print(w)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.7677718   3.71535193 -1.14722916]\n"
     ]
    }
   ],
   "execution_count": 32
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
