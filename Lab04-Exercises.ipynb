{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Given the following dataset with two input random variables $X_1$ and $X_2$ and a target variable $Y$, we want to compare two extreme decision tree algorithms:\n",
    "\n",
    "* OVERFIT will build a full standard ID3 decision tree, with no pruning;\n",
    "* UNDERFIT will make no splits at all, always having a single node (which is both root and decision).\n",
    "\n",
    "1. Plot the full OVERFIT tree.\n",
    "1. What is the CVLOO error for OVERFIT?\n",
    "1. What is the CVLOO error for UNDERFIT?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "d = pd.DataFrame({'X1': [1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8],\n",
    "                  'X2': [1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2],\n",
    "                  'Y' : [0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Suppose we learned a decision tree from a training set with binary output values (either 0 or 1). We find that for a leaf node $l$, \n",
    "\n",
    "* there are $M$ training examples falling into it (labeled either 0 or 1); \n",
    "* its entropy is $H$. \n",
    "\n",
    "1. Create a graph using `matplotlib` that shows the entropy $H$ as a function of the proportion of 1s in $M$. The proportion should be on the $x$ axis (from 0 to 1), while the entropy should be on the $y$ axis.\n",
    "1. Create a simple algorithm which takes as input $M$ and $H$ and that outputs the number of training examples misclassified by leaf node $l$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Given the dataset below:\n",
    "1. plot the points and the labels using `matplotib.pyplot.scatter`;\n",
    "1. train a regular decision tree, then plot its decision surface;\n",
    "1. create a new dataset with 1000 random points with coordinates between 0 and 10, which the diagonal line $X1 = X2$ perfectly separates in two classes. See [numpy.random.random_sample](https://numpy.org/doc/stable/reference/random/generated/numpy.random.random_sample.html#numpy.random.random_sample) for easily generating random numbers between 0 and 1.\n",
    "1. train a regular decision tree, then plot its decision surface on the new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "d = pd.DataFrame({'X1': [1, 2, 3, 3, 3, 4, 5, 5, 5],\n",
    "                  'X2': [2, 3, 1, 2, 4, 4, 1, 2, 4],\n",
    "                  'Y':  [1, 1, 0, 0, 0, 0, 1, 1, 0]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
